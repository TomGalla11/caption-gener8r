{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests as rq\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "# from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import (LSTM, Embedding, Input, BatchNormalization, \n",
    "                          Dense, RepeatVector, Concatenate, \n",
    "                          TimeDistributed, Dropout)\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "# from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process images\n",
    "# cnn_model = VGG16(include_top=False, weights='imagenet')\n",
    "cnn_model = VGG16()\n",
    "# re-structure the model\n",
    "cnn_model.layers.pop()\n",
    "cnn_model = Model(inputs=cnn_model.inputs, outputs=cnn_model.layers[-1].output)\n",
    "num_images = 500\n",
    "\n",
    "data = pd.read_csv(\"masterdata.csv\")\n",
    "images = []\n",
    "features = []\n",
    "image_dim = 224\n",
    "for url in data.photo[:num_images]:\n",
    "    response = rq.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).resize((image_dim,image_dim))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "    x = preprocess_input(x)\n",
    "    f = cnn_model.predict(x)\n",
    "#     images.append(x)\n",
    "    features.append(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get captions\n",
    "captions = data.caption[:num_images]\n",
    "captions = [c.replace('\\n', ' ') for c in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(lower=False, char_level=True,filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(captions)\n",
    "encoded_captions = tokenizer.texts_to_sequences(captions)\n",
    "start = len(tokenizer.word_index) + 1\n",
    "stop = start + 1\n",
    "vocab_size = stop + 1\n",
    "\n",
    "encoded_captions = [([start] + c) for c in encoded_captions]\n",
    "encoded_captions = [(c + [stop]) for c in encoded_captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cap = max(len(c) for c in encoded_captions)\n",
    "X1 = []\n",
    "X2 = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(encoded_captions)):\n",
    "    c = encoded_captions[i]\n",
    "    for j in range(len(c)):\n",
    "        in_seq, out_seq = c[:j], c[j]\n",
    "        in_seq = pad_sequences([in_seq], max_cap)[0]\n",
    "        out_seq = to_categorical(out_seq, num_classes = vocab_size)\n",
    "        X1.append(features[i])\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "X1 = np.reshape(X1,(np.shape(X1)[0], np.shape(X1)[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = stop + 1\n",
    "\n",
    "# feature extractor model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# sequence model\n",
    "inputs2 = Input(shape=(max_cap,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# tie it together [image, seq] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# summarize model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_data = int(len(X1) * 4 / 5)\n",
    "\n",
    "X1train = np.array(X1[:amt_data])\n",
    "X1test = np.array(X1[amt_data:])\n",
    "\n",
    "X2train = np.array(X2[:amt_data])\n",
    "X2test = np.array(X2[amt_data:])\n",
    "\n",
    "ytrain = np.array(y[:amt_data])\n",
    "ytest = np.array(y[amt_data:])\n",
    "\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=1, validation_data=([X1test, X2test], ytest))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_letter(yhat):\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v == yhat:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(3):\n",
    "    i = randint(0, num_images)\n",
    "    print(captions[i])\n",
    "    pred_encoded = [start]\n",
    "    f = np.array(features[i])\n",
    "    pred_pad = np.array(pad_sequences([pred_encoded], max_cap))\n",
    "    \n",
    "    yhat = model.predict([f, pred_pad])\n",
    "    yhat = np.argmax(yhat)    \n",
    "    pred_encoded.append(yhat)\n",
    "    pred_pad = np.array(pad_sequences([pred_encoded], max_cap))\n",
    "    pred_capt = to_letter(yhat)\n",
    "    \n",
    "    while yhat != stop and len(pred_capt) < 100:\n",
    "        yhat = model.predict([f, pred_pad])\n",
    "        yhat = np.argmax(yhat)\n",
    "        if yhat is start or yhat is stop:\n",
    "            break\n",
    "        pred_encoded.append(yhat)\n",
    "        pred_pad = np.array(pad_sequences([pred_encoded], max_cap))\n",
    "        pred_capt += to_letter(yhat)\n",
    "    print(pred_capt)\n",
    "    print()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
